seed_everything: 123

trainer:
  #devices: 4
  #accumulate_grad_batches: 2
  accelerator: gpu
  precision: 16-mixed
  #strategy: "ddp_find_unused_parameters_true"
  max_epochs: 100
  gradient_clip_val: 1.0

  # callbacks:
  #   - class_path: pytorch_lightning.callbacks.ModelCheckpoint
  #     init_args:
  #       monitor: val_loss
  #       dirpath: ttc/checkpoints/${trainer.logger.init_args.name}
  #       filename: checkpoint_{epoch}_{step}_{val_loss:.4f}
  #       save_top_k: 3
  #       save_last: true
  
  
  # logger:
  #   class_path: pytorch_lightning.loggers.TensorBoardLogger
  #   init_args:
  #     save_dir: logs
  #     name: lina_ljspeech #_gla

data:
  class_path: dataset.ManifestDataModule
  init_args:
    root_path: "feats_LJSpeech-1.1"
    train_manifest_path: "ljspeech_feats_manifest_rwkv_v2.json"
    env_root: "DBROOT"
    quant_layer: [0, 1]
    batch_size: 32
    symbols_file: "symbols.txt"
    phon_col: "norm.phonemized.txt"

model:
  class_path: model.lina.Lina
  init_args:
    n_codebook: 1024
    n_special_token_in: 3
    n_special_token_out: 3
    n_txt_vocab: 180
    d_context: 256
    d_model: 256
    txt_encoder:
      class_path: model.encoder.TextEncoder
      init_args:
        dim: ${model.init_args.d_context}
        heads: 4
        n_layers: 4
        dropout: 0.1
    attentive_rnn:
      class_path: model.mamba.AttentiveMamba
      init_args:
        d_model: ${model.init_args.d_model}
        d_context: ${model.init_args.d_context}
        heads: 1
        dropout_att: 0.1
        n_layer: 3
        blind: False
