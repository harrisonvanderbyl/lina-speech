seed_everything: 123

trainer:
  accumulate_grad_batches: 1
  accelerator: gpu
  precision: bf16
  max_epochs: 100
  gradient_clip_val: 1.0

  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        dirpath: checkpoints/${trainer.logger.init_args.name}
        filename: checkpoint_{epoch}_{step}_{val_loss:.4f}
        save_top_k: 3
        save_last: true

  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: ttc/logs
      name: vctk

data:
  class_path: ttc.dataset.ManifestDataModule
  init_args:
    root_path: "feats_VCTK-Corpus-0.92"
    train_manifest_path: "vctk_feats_manifest_rwkv.json"
    env_root: "DBROOT"
    quant_layer: [0, 1]
    batch_size: 16
    symbols_file: "symbol.txt"

model:
  class_path: model.lina.Lina
  init_args:
    n_codebook: 1024
    n_special_token_in: 3
    n_special_token_out: 3
    n_txt_vocab: 180
    d_context: 256
    d_model: 256
      #attentive_rnn:
      #  class_path: ttc.gla.AttentiveGLA
      #  init_args:
      #    d_model: ${model.init_args.d_model}
      #    d_context: ${model.init_args.d_context}
      #    att_heads: 1
      #    gla_heads: 2
      #    dropout_att: 0.1
      #    n_layer: 3
    txt_encoder:
      class_path: model.encoder.TextEncoder
      init_args:
        dim: ${model.init_args.d_context}
        heads: 4
        n_layers: 4
        dropout: 0.1
    attentive_rnn:
      class_path: model.mamba.AttentiveMamba
      init_args:
        d_model: ${model.init_args.d_model}
        d_context: ${model.init_args.d_context}
        heads: 1
        dropout_att: 0.1
        n_layer: 6
 
       
      
          #rwkv_config:
          #  block_size: 2048
          #  n_layer: 6
          #  n_embd: 256
          #  dropout: 0.1
          #  heads: 2
          #  attentive_layer: [2,3]
          #  context_dim: ${model.init_args.context_dim}
        
